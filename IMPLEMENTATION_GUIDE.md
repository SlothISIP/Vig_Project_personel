# ğŸš€ Implementation Guide: Getting Started

> **"ultrathink" ëª¨ë“œë¡œ ì„¤ê³„ëœ í”„ë¡œì íŠ¸ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œ**

ì´ ë¬¸ì„œëŠ” ì„¤ê³„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ì¤€ë¹„ ì‚¬í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- [ ] GPU: NVIDIA GPU with 6GB+ VRAM (ê¶Œì¥: RTX 3060 ì´ìƒ)
  - CPU onlyë¡œë„ ê°€ëŠ¥í•˜ë‚˜ í•™ìŠµ ì†ë„ê°€ 10-20ë°° ëŠë¦¼
- [ ] RAM: 16GB+ (ê¶Œì¥: 32GB)
- [ ] ë””ìŠ¤í¬: 100GB+ ì—¬ìœ  ê³µê°„ (ë°ì´í„°ì…‹ + ëª¨ë¸)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- [ ] OS: Ubuntu 20.04+ / macOS 12+ / Windows 10+ (WSL2)
- [ ] Python: 3.10 or 3.11
- [ ] CUDA: 11.8+ (GPU ì‚¬ìš© ì‹œ)
- [ ] Git: ìµœì‹  ë²„ì „
- [ ] Docker & Docker Compose (ì„ íƒì‚¬í•­)

### ê³„ì • ì¤€ë¹„
- [ ] GitHub ê³„ì • (ì½”ë“œ ë²„ì „ ê´€ë¦¬)
- [ ] Weights & Biases ê³„ì • (ì„ íƒ - ì‹¤í—˜ ì¶”ì )
- [ ] AWS/GCP ê³„ì • (ì„ íƒ - í´ë¼ìš°ë“œ ë°°í¬)

---

## ğŸ¬ Step 1: í”„ë¡œì íŠ¸ ìƒì„± ë° ì´ˆê¸°í™”

### 1.1 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±

```bash
# ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/projects/digital-twin-factory
cd ~/projects/digital-twin-factory

# Git ì´ˆê¸°í™”
git init
git branch -M main

# GitHubì— ìƒˆ repository ìƒì„± í›„ ì—°ê²°
git remote add origin https://github.com/YOUR_USERNAME/digital-twin-factory.git
```

### 1.2 ì„¤ê³„ ë¬¸ì„œ ë³µì‚¬

í˜„ì¬ `Vig_Project_personel` í´ë”ì˜ ë‹¤ìŒ ë¬¸ì„œë“¤ì„ í”„ë¡œì íŠ¸ë¡œ ë³µì‚¬:

```bash
# ì„¤ê³„ ë¬¸ì„œ ë³µì‚¬ (ì´ ë¬¸ì„œë“¤ì´ ìˆëŠ” ìœ„ì¹˜ì—ì„œ ì‹¤í–‰)
cp ARCHITECTURE.md ~/projects/digital-twin-factory/docs/
cp PROJECT_STRUCTURE.md ~/projects/digital-twin-factory/docs/
cp WEEK1_PLAN.md ~/projects/digital-twin-factory/docs/
cp README.md ~/projects/digital-twin-factory/
cp Makefile ~/projects/digital-twin-factory/
cp docker-compose.yml ~/projects/digital-twin-factory/
cp .env.example ~/projects/digital-twin-factory/
cp .gitignore ~/projects/digital-twin-factory/
```

### 1.3 ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
cd ~/projects/digital-twin-factory

# PROJECT_STRUCTURE.mdì˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p {src,tests,data,models,notebooks,config,scripts,docs,deploy,frontend}
mkdir -p src/{core,vision,digital_twin,scheduling,data,api,workers,iot,utils}
mkdir -p src/vision/{models,preprocessing,postprocessing,inference,training,utils}
mkdir -p src/digital_twin/{state,simulation,predictive,events}
mkdir -p src/scheduling/{algorithms,constraints,objectives}
mkdir -p src/data/{database,repositories,cache,storage}
mkdir -p src/api/{routes,schemas,services,middleware}
mkdir -p src/workers/{tasks,schedulers}
mkdir -p src/iot/{mqtt,amqp,simulators}
mkdir -p tests/{unit,integration,e2e,performance}
mkdir -p data/{raw,processed,annotations}
mkdir -p models/{checkpoints,onnx,tensorrt,mlflow}
mkdir -p deploy/{docker,kubernetes,terraform,ansible,monitoring}
mkdir -p config/models

# __init__.py íŒŒì¼ ìƒì„±
find src -type d -exec touch {}/__init__.py \;
find tests -type d -exec touch {}/__init__.py \;

echo "âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ"
```

### 1.4 .gitignore ìƒì„±

```bash
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
ENV/
env/
.venv

# Poetry
poetry.lock

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb

# PyTorch
*.pth
*.pt
*.ckpt

# Data
data/raw/
data/processed/
!data/.gitkeep

# Models
models/checkpoints/
models/onnx/
models/tensorrt/
!models/.gitkeep

# MLflow
mlruns/
mlflow.db

# DVC
.dvc/cache

# Logs
*.log
logs/

# Environment variables
.env
.env.local

# Coverage
.coverage
htmlcov/
.pytest_cache/

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# ruff
.ruff_cache/

# Docker
.dockerignore

# Temporary files
*.tmp
*.bak
.cache/
EOF
```

---

## ğŸ Step 2: Python í™˜ê²½ ì„¤ì •

### 2.1 Poetry ì„¤ì¹˜

```bash
# Poetry ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
curl -sSL https://install.python-poetry.org | python3 -

# PATH ì¶”ê°€ (zsh ì‚¬ìš© ì‹œ)
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc

# bash ì‚¬ìš© ì‹œ
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

# ì„¤ì¹˜ í™•ì¸
poetry --version
```

### 2.2 pyproject.toml ìƒì„±

```bash
cat > pyproject.toml << 'EOF'
[tool.poetry]
name = "digital-twin-factory"
version = "0.1.0"
description = "AI-Driven Digital Twin Factory System with Vision Transformer"
authors = ["Your Name <your.email@example.com>"]
license = "MIT"
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.10"
# Core
fastapi = "^0.104.0"
uvicorn = {extras = ["standard"], version = "^0.24.0"}
pydantic = "^2.4.0"
pydantic-settings = "^2.0.0"

# Vision AI
torch = "^2.1.0"
torchvision = "^0.16.0"
timm = "^0.9.0"
onnx = "^1.15.0"
onnxruntime = "^1.16.0"
opencv-python = "^4.8.0"
albumentations = "^1.3.0"
Pillow = "^10.1.0"

# Database
sqlalchemy = "^2.0.0"
asyncpg = "^0.29.0"
psycopg2-binary = "^2.9.0"
redis = "^5.0.0"
alembic = "^1.12.0"

# ML Ops
mlflow = "^2.8.0"
dvc = {extras = ["s3"], version = "^3.30.0"}
optuna = "^3.4.0"

# Scheduling
ortools = "^9.8.0"
scipy = "^1.11.0"
numpy = "^1.26.0"

# Workers
celery = {extras = ["redis"], version = "^5.3.0"}
pika = "^1.3.0"

# IoT
paho-mqtt = "^1.6.0"

# Monitoring
prometheus-client = "^0.18.0"

# Utils
python-dotenv = "^1.0.0"
pyyaml = "^6.0.0"
click = "^8.1.0"
python-multipart = "^0.0.6"
passlib = {extras = ["bcrypt"], version = "^1.7.0"}
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
pandas = "^2.1.0"
matplotlib = "^3.8.0"
seaborn = "^0.13.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.21.0"
pytest-cov = "^4.1.0"
black = "^23.10.0"
ruff = "^0.1.0"
mypy = "^1.6.0"
ipython = "^8.16.0"
jupyter = "^1.0.0"
pre-commit = "^3.5.0"

[tool.poetry.group.test.dependencies]
httpx = "^0.25.0"
locust = "^2.17.0"
faker = "^19.13.0"

[tool.black]
line-length = 100
target-version = ['py310']
include = '\.pyi?$'

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W", "UP"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.10"
strict = false
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
addopts = "--cov=src --cov-report=html --cov-report=term -v"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
EOF
```

### 2.3 ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ëª¨ë“  ì˜ì¡´ì„± ì„¤ì¹˜ (ê°œë°œ ë„êµ¬ í¬í•¨)
poetry install

# ê°€ìƒí™˜ê²½ í™œì„±í™”
poetry shell

# ì„¤ì¹˜ í™•ì¸
python --version
pip list | grep torch
```

---

## ğŸ“¥ Step 3: ë°ì´í„°ì…‹ ì¤€ë¹„

### 3.1 ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

```bash
cat > scripts/download_datasets.py << 'EOF'
#!/usr/bin/env python3
"""Download MVTec AD and DAGM datasets"""

import urllib.request
import tarfile
import zipfile
from pathlib import Path
from tqdm import tqdm

def download_mvtec_ad():
    """Download MVTec Anomaly Detection dataset"""
    base_url = "https://www.mvtec.com/company/research/datasets/mvtec-ad"
    categories = [
        'bottle', 'cable', 'capsule', 'carpet', 'grid',
        'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',
        'tile', 'toothbrush', 'transistor', 'wood', 'zipper'
    ]

    data_dir = Path('data/raw/mvtec_ad')
    data_dir.mkdir(parents=True, exist_ok=True)

    print("ğŸ“¥ Downloading MVTec AD dataset...")
    print("âš ï¸  NOTE: MVTec AD requires manual download from official website")
    print(f"Please download from: {base_url}")
    print(f"Extract all categories to: {data_dir}")
    print("\nAfter download, your structure should be:")
    print("  data/raw/mvtec_ad/")
    print("    â”œâ”€â”€ bottle/")
    print("    â”œâ”€â”€ cable/")
    print("    â””â”€â”€ ...")

if __name__ == "__main__":
    download_mvtec_ad()
EOF

chmod +x scripts/download_datasets.py
python scripts/download_datasets.py
```

**Manual Download Steps:**
1. MVTec AD ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸: https://www.mvtec.com/company/research/datasets/mvtec-ad
2. ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‹¤ìš´ë¡œë“œ (ë˜ëŠ” í•„ìš”í•œ ê²ƒë§Œ)
3. `data/raw/mvtec_ad/` ì— ì••ì¶• í•´ì œ

### 3.2 ë°ì´í„° ê²€ì¦

```bash
cat > scripts/verify_data.py << 'EOF'
#!/usr/bin/env python3
"""Verify dataset structure and integrity"""

from pathlib import Path

def verify_mvtec():
    data_dir = Path('data/raw/mvtec_ad')

    if not data_dir.exists():
        print("âŒ MVTec AD dataset not found!")
        return False

    categories = list(data_dir.iterdir())
    print(f"âœ… Found {len(categories)} categories")

    for cat in categories[:3]:  # Check first 3
        train_good = cat / 'train' / 'good'
        test_dir = cat / 'test'

        if train_good.exists() and test_dir.exists():
            n_train = len(list(train_good.glob('*.png')))
            print(f"  {cat.name}: {n_train} training images")
        else:
            print(f"  âŒ {cat.name}: Invalid structure")
            return False

    return True

if __name__ == "__main__":
    if verify_mvtec():
        print("\nâœ… Dataset verification passed!")
    else:
        print("\nâŒ Dataset verification failed!")
EOF

python scripts/verify_data.py
```

---

## ğŸ‹ï¸ Step 4: ì²« ëª¨ë¸ í•™ìŠµ (Day 1-2 ì‘ì—…)

### 4.1 ê¸°ë³¸ ëª¨ë¸ ì½”ë“œ ìƒì„±

```bash
# Vision ëª¨ë¸ êµ¬í˜„
cat > src/vision/models/swin_transformer.py << 'EOF'
import torch
import torch.nn as nn
import timm

class SwinDefectDetector(nn.Module):
    """Swin Transformer for defect detection"""

    def __init__(
        self,
        model_name: str = 'swin_tiny_patch4_window7_224',
        num_classes: int = 2,
        pretrained: bool = True
    ):
        super().__init__()

        # Load pretrained Swin Transformer
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=0  # Remove head
        )

        # Custom classification head
        self.feature_dim = self.backbone.num_features
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        features = self.backbone(x)
        logits = self.classifier(features)
        return logits
EOF

# í…ŒìŠ¤íŠ¸
python -c "
from src.vision.models.swin_transformer import SwinDefectDetector
model = SwinDefectDetector()
print(f'âœ… Model created: {sum(p.numel() for p in model.parameters()):,} parameters')
"
```

### 4.2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ê°„ë‹¨ ë²„ì „)

```bash
cat > scripts/train_simple.py << 'EOF'
#!/usr/bin/env python3
"""Simple training script for testing setup"""

import torch
from src.vision.models.swin_transformer import SwinDefectDetector

def main():
    print("ğŸš€ Starting simple training test...")

    # Create model
    model = SwinDefectDetector(num_classes=2, pretrained=True)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    print(f"âœ… Model on device: {device}")
    print(f"âœ… Parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Test forward pass
    dummy_input = torch.randn(4, 3, 224, 224).to(device)
    with torch.no_grad():
        output = model(dummy_input)

    print(f"âœ… Forward pass successful: {output.shape}")
    print("\nâœ… Setup verification complete!")
    print("\nNext steps:")
    print("1. Download dataset (see WEEK1_PLAN.md)")
    print("2. Run full training: python scripts/train_baseline.py")

if __name__ == "__main__":
    main()
EOF

python scripts/train_simple.py
```

---

## ğŸ³ Step 5: Docker í™˜ê²½ ì„¤ì • (ì„ íƒì‚¬í•­)

### 5.1 Dockerfile ìƒì„±

```bash
mkdir -p deploy/docker

cat > deploy/docker/Dockerfile.api << 'EOF'
FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -
ENV PATH="/root/.local/bin:$PATH"

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
RUN poetry config virtualenvs.create false \
    && poetry install --no-root --only main

# Copy application code
COPY src/ ./src/
COPY config/ ./config/

EXPOSE 8000

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF
```

### 5.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ì¤‘ìš”!)
sed -i '' 's/your_secure_password_here/MySecurePass123!/g' .env
sed -i '' 's/your_rabbitmq_password_here/RabbitMQPass123!/g' .env
sed -i '' 's/your_minio_password_here/MinIOPass123456!/g' .env
```

### 5.3 Docker Composeë¡œ ì „ì²´ ìŠ¤íƒ ì‹¤í–‰

```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f api

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ì ‘ì† í…ŒìŠ¤íŠ¸
curl http://localhost:8000/health
```

---

## âœ… Step 6: ì„¤ì • ì™„ë£Œ ê²€ì¦

ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸:

```bash
cat > scripts/verify_setup.sh << 'EOF'
#!/bin/bash

echo "ğŸ” Verifying setup..."

# Check Python
if python --version | grep -q "3.10\|3.11"; then
    echo "âœ… Python version OK"
else
    echo "âŒ Python version incorrect"
fi

# Check Poetry
if poetry --version &> /dev/null; then
    echo "âœ… Poetry installed"
else
    echo "âŒ Poetry not found"
fi

# Check CUDA (if available)
if python -c "import torch; print(torch.cuda.is_available())" | grep -q "True"; then
    echo "âœ… CUDA available"
else
    echo "âš ï¸  CUDA not available (CPU only)"
fi

# Check directory structure
if [ -d "src" ] && [ -d "tests" ] && [ -d "data" ]; then
    echo "âœ… Directory structure OK"
else
    echo "âŒ Directory structure incomplete"
fi

# Check dependencies
if poetry run python -c "import torch, timm, fastapi" &> /dev/null; then
    echo "âœ… Key dependencies installed"
else
    echo "âŒ Dependencies missing"
fi

echo ""
echo "ğŸ‰ Setup verification complete!"
EOF

chmod +x scripts/verify_setup.sh
./scripts/verify_setup.sh
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´ `WEEK1_PLAN.md`ë¥¼ ë”°ë¼ ê°œë°œì„ ì‹œì‘í•˜ì„¸ìš”:

### Day 1 ì‘ì—…:
1. âœ… í™˜ê²½ ì„¤ì • (ì™„ë£Œ!)
2. ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
3. ğŸ“Š ë°ì´í„° íƒìƒ‰ (Jupyter Notebook)
4. ğŸ”§ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### Day 2 ì‘ì—…:
1. ğŸ‹ï¸ Baseline ëª¨ë¸ í•™ìŠµ
2. ğŸ“ˆ MLflow ì‹¤í—˜ ì¶”ì 
3. ğŸ“Š ëª¨ë¸ í‰ê°€

### Day 3 ì‘ì—…:
1. âš¡ ONNX ìµœì í™”
2. ğŸŒ FastAPI ì—”ë“œí¬ì¸íŠ¸
3. ğŸ§ª ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **Architecture**: `docs/ARCHITECTURE.md`
- **Project Structure**: `docs/PROJECT_STRUCTURE.md`
- **Week 1 Plan**: `docs/WEEK1_PLAN.md`
- **API Documentation**: http://localhost:8000/docs (after starting server)
- **MLflow UI**: http://localhost:5000 (after starting MLflow)

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### CUDA out of memory
```bash
# batch size ì¤„ì´ê¸°
# scripts/train_baseline.pyì—ì„œ:
batch_size = 16  # 32ì—ì„œ 16ìœ¼ë¡œ
```

### Poetry install ì‹¤íŒ¨
```bash
# Cache ì‚­ì œ í›„ ì¬ì‹œë„
poetry cache clear pypi --all
poetry install
```

### Docker ë¹Œë“œ ì‹¤íŒ¨
```bash
# ìºì‹œ ì—†ì´ ì¬ë¹Œë“œ
docker-compose build --no-cache
```

### ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ
# https://www.mvtec.com/company/research/datasets/mvtec-ad
```

---

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.**

ì´ì œ `WEEK1_PLAN.md`ë¥¼ ë”°ë¼ í”„ë¡œì íŠ¸ ê°œë°œì„ ì‹œì‘í•˜ì„¸ìš”!
